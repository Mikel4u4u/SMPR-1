#Павлюков В.В. 401и
#source("PRozenblata.R")
N = c(1,3)#вектор признаков, содержит номера признаков учавствующих в обучении
Nc = c(1,2)#порядковые номера классов учавствующих в классификации
source("PRozenblata_functions.R")
#Вход:
  #обучающая выборка
    Xl=f_to_M(iris,N,Nc)
    row=dim(Xl)[1]
    col=dim(Xl)[2]
  #нормировка признаков
    ma=max_elements(Xl)
    mi=min_elements(Xl)
    X0<-Xl#копия выборки,чтобы не делать обратный пересчет из нормированой
    Xl=norm(Xl,ma,mi)#нормировка признаков(по столбцам кроме последнего)
#получение весов
  #1:инициализировать начальные веса
    w0=0#w0 в нормированной выборке равен нулю
    w1<<-rnorm(col-1)/(2*row)#предыдущее значение параметров алгоритма
  #2:инициализировать текущую оценку функционала
    start_Q(10)#задаем спец. стек для проверки стабилизации Q
    app_Q(Q(Xl,wi,w0))#Заносит значение функционала в специальный стек
#подготовка к циклу
  iteration<-1#счетчик итераций
  wi=w1#текущее Значение параметров алгоритма
  start_next_N()#подготовка функции выбора элементов
repeat{
  #4:выбрать объект х
  i={
    b=c(0)#вектор номеров неправильно классифицированных элементов
    for(k in 1:row){
      if((abs(Xl[k,col]-a(as.double(Xl[k,][-col]),wi,w0))/2)!=0){
        b=append(b,k)
      }
    }
    if(length(b)==1){
      break#выборка разделена
    }else{
      sample(b[-1],1,replace=FALSE)#извлечение случайного элемента
    }
  }
  xi<-as.double(Xl[i,][-col])#признаки k-го элемента
  yi<-Xl[i,col]
  #5:вычислить выходное значение алгоритма a(xi,w)
  y<-a( xi,w1,w0 )
  #6:сделать шаг градиентного спуска:
  if(y==yi){
    iteration=iteration+1
    next
  }else{
    if(y==1){
      wi=wi-Eta(iteration)*xi
    }else{
      wi=wi+Eta(iteration)*xi
    }
  }
  #7:оценить значение функционала:
  app_Q(Q(Xl,wi,w0))#Заносит значение функционала в специальный стек
  iteration<-iteration+1
  #8:пока значение Q не стабилизируется и/или веса не перестанут изменяться
  if(stub_Q()<1e-5){#сумма абсолютных отклонений вектора от его среднего
    break
  }else{
    w1=wi
    next
  }
}
# график
Xl=X0#возврат выборки в исходное состояние
#пересчет параметров алгоритма
wi=wi/((ma-mi)/2)
w0=wi%*%((mi+ma)/2)
#процент ошибок
e=0
for(n in 1:row){
  e=e+abs(Xl[n,col]-a(as.double(Xl[n,][-col]),wi,w0))/2
}
e=100*e/row
#график распределения
tcveta = c("red", "blue", "green")#создает вектор цветов
#функция проходящая через точки (1,1),(0,3),(-1,2)
Ntc=function(k){
  ifelse(k==1,1,3+k)
}
plot(Xl[,1], Xl[,2], col = tcveta[Ntc(Xl[,col])], xlab = "", ylab = "",
     main = paste("PERSEPTRON\n(error rate ",round(e,digits=2),"%, iterations ",iteration,")"),pch = 20)
#классифицируем все точки видимой области первого графика с шагом (0.1, 0.1)    
w_min<-min(Xl[,1])-0.1
w_max<-max(Xl[,1])+0.1
h_min<-min(Xl[,2])-0.1
h_max<-max(Xl[,2])+0.1
we<-w_min
while(we<w_max){
  h=h_min
  while(h<h_max){
    points(we, h, col = tcveta[Ntc(a(c(we,h),wi,w0 ))], pch = 1)
    h=h+0.1
  }
  we=we+0.1
}
